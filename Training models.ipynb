{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold, cross_val_score, validation_curve\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the model can help decide the right training model to use, a good set of hyperparemeters for your task.and perform the error analysis more efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two ways to train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Using the 'closed form' equation that directly computes the best parameter that minimizes the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using Gradient descent(GD),And iterative optimization approach.That gradually tweaks the model parameter to minimize the cost function the cost function over the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='linreg.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This can be written more concisely using a vectorized form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='linvec.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we train it? The best way of training a model means setting its parameters so thatthe model best fits the training set. for this we need a cost function to check how poorly the model is performing.For this the most common is RMSE in linear regression but in practice it is easier to find the value of theta for MSE that minimizes the cost function. The values of theta that minimizes the MSE also minimizes the RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE MSE of a linear regression hypothesis theta not on a training set X is calculated using the following equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='MSE.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NORMAL EQUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to find the value of theta that minizes the MSE. We can use 'closed form' equation. It gives the the value of theta directly.This is called the normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='normal.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X=2*np.random.rand(100,1)\n",
    "y=4+3*X+np.random.randn(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.81958997]), array([[3.26763971]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(X,y)\n",
    "lin_reg.intercept_,lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The disadvantage if Normal equation is it computes the inverse of the transpose. The matrix is N*N therefore the computational cost is O(n^3).Thereofre if we doubles the number of features then it increses the computational cost by 8.  2^3=8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But on the positive side this equation is linear with regard of number of instances in the training set(it is O(m)).so it handles the large training set efficiently provided it can fit in the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GD is better suited for the case whe the features and instaces are large to fit in meomery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GD is avery generic optimization algo capable of finding wide range of problems. The generic idea of GD is to tweak parameter iteratively in order to minimize a cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It measures the local gradient of the error function with regards to the parameter vector theta.and it goes in the direction of descending gradient.once gradient is zero,you have reached the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important parameter in gradiet descent is the size of the steps.determined by the learning rate hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to deal with the local minimum problem. Fortunately MSE cost function is convex function.This implies that there are no local minima,just one global minimum.Therefore gradient descent is guaranteed to approach the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While using gradient descent we should ensure that all features have a similar scale.Or else it will take much longer to converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the gradient descent.you need to compute the gradient of the cost function with regard to the each model parameter theta.ie, WE NEED TO  calculate how much the cost function will change if you change theta little bit. This is called partial derivatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='partial.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instead of computing these partial derivatives individually. We can use gradient vector to compute partial derivatives all in one go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='vc.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b=np.c_[np.ones((100,1)),X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This formula involves calculations over the full training set X, at each Gradient Descent. Therefore makes this algorithm terrible slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta=0.1 #learning rate\n",
    "n_iterations= 100\n",
    "m=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta= np.random.randn(2,1) #random initialization\n",
    "for iteration in range(n_iterations):\n",
    "    gradients=2/m*X_b.T.dot(X_b.dot(theta)-y)\n",
    "    theta =theta-eta*gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.65896591],\n",
       "       [3.40320691]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to decide the learning rate? <br>\n",
    "grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to decide the number of iterations?<br>\n",
    "If we choose really small then it will be far behind from the minimum. And if it is very high then it will vross the minimum value. SO the best approach is to choose the very high value and inturrupt the algorithm when the gradient vector becomes tiny-that is, when its norm becomes smaller than a tiny norm, called tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem with the bathc gradient descent is that. IT TAKES  full training set set at every step, makes it very slow. In opposite extreme stochastic(random)gradient descent picks the random instance in the training set. And computes gradient descent based on that single instance. therefore it is faster than the batch graduent descent. But instead of decreasing gradually it changes values abruptly and reaches minimum faster but du to the randomness nature it keeps on bouncing near the minimum. So it is ALMOST minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the cost function is very irregular,this can actually help from bouncing from the local minima.Therefore the irregualr cost function has better chance to find the global minima.but negative side is that it can never settle down to the minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but how can it negative settle down to the minimum?<br>\n",
    "We can solve this problem by gradually reducing the learning rate. start bwith the large value and with every step it gets smaller and smaller and helps in settling in gloabal minima. This process is known as simulared annealing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=50\n",
    "t0,t1=5,50 #learning schedule hyperparameters\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "theta=np.random.randn(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d61dafe21924>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0myi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrandom_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mgradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0myi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epch' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index=np.random.randint(m)\n",
    "        xi=X_b[random_index:random_index+1]\n",
    "        yi=y[random_index:random_index+1]\n",
    "        gradients=2*xi.T.dot(xi.dot(theta)-yi)\n",
    "        eta=learning_schedule(epch*m+i)\n",
    "        theta=theta-eta*gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.75578949],\n",
       "       [-2.26323394]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m=iteration round each round called an epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the instances are picked randomly,some instacnes may be picked several times per epoch while other may not be picked at all.How do we resolve this<br>\n",
    "if we want to ensure that algorithm goes through every instance at each epoch,another approach is to shuffle the training set,then go instance by instance,then shuffle it again, and so on.this generally converges more slowly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform linear regression using stochastic gradient descent. We can use SGDRegressor class. which defaults to optimizing the mean squared error cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_iter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8472a96d56c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGDRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msgd_reg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSGDRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meta0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msgd_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_iter'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg=SGDRegressor(n_iter=50,penalty=None,eta0=0.1)\n",
    "sgd_reg.fit(X,y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eta0=learning rate<br>\n",
    "n_iter=epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mini batch Gradient Descent. instead of computing the gradient descent based on full training set or on single instance,Mini batch GD computes the gradient descent on small random sets of instance called mini batch. The main advantage is that we can boost fro hardware optimization of matrix operation,If we are using GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MINI BATCH GD. is less erratic then SGD. therefore it be closer to the minimum then the SGD,But on the other handit may be harder for it to escape the local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='comp.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if your data is more complex than a simple straight line? Surprisingly, you can actually use a lnear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features.This techniques is called poltnomial regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets generate some nonlinear data,based on a simple quadratic equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=100 \n",
    "x=6*np.random.rand(m,1)-3\n",
    "y=0.5*x**2+x+2+np.random.randn(m,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearly, a straight line will never fit this data properly.So lets use scikit learn's polynomialFeatures class to transform our training set. It can be done by passing degree =2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88207555])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features=PolynomialFeatures(degree=2,include_bias=False)\n",
    "X_poly=poly_features.fit_transform(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88207555, 0.77805728])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_poly[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOw X_poly contains the progonal feature of X plus the square of this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.68203091]), array([[ 2.03534785, -0.85680094]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(X_poly,y)\n",
    "lin_reg.intercept_,lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model estimates y hat=(-.85x1)^2(+2.03x1)+2.68."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polynomial regression is capable of finding the combination of the features. If we have the feature a and b with degree 2.It would not only add a^2,b^2,but also the combination of ab,ab^2,a^2b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High degree likely to cause overfitting and plain linear model likely to create underfiting<br>\n",
    "Then how do we decide what degree to choose?<br>\n",
    "if model works well on the training data and works bad on validation then it is the case of overfitting.But if the model works poor on both then it is the case of underfiting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to decide is to use Learning curves: These are the plots of the model performance on the training set and the validation set as a function of the training set size.To generate the model simply train the model several times on different sized subsets of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train_predict, y_train[:m])) \n",
    "        val_errors.append(mean_squared_error(y_val_predict, y_val))\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAdiElEQVR4nO3deXRc9X338fdXu7wvko03YqCE5WGxQRAXHggBTNgStrbAEzhulpLwkMPaLCyNTQMNSdhPCS0tJD5PUkPLjoEElzihTTgYgY0xNmAwDmC7tmzL8iLJlma+zx+/K81IGlm7NNf38zrnnpl7587MdxZ95qvfvTPX3B0REYmfgqEuQEREekcBLiISUwpwEZGYUoCLiMSUAlxEJKaKBvPOKioqfPr06YN5lyIisffGG29sdvfK9ssHNcCnT59OdXX1YN6liEjsmdmfci3XEIqISEwpwEVEYkoBLiISUwpwEZGY6jLAzazMzJaY2Vtm9o6Z3RotP8DMXjOz1Wb2mJmVDHy5IiLSojsd+G7gVHc/GpgBnGlms4AfA/e4+8FALfD1gStTRETa6zLAPdgZzRZHkwOnAo9Hy+cD5w9IhTnU1sL69YN1byIi+albY+BmVmhmy4BNwCLgQ2CbuzdHq3wKTOnkuleYWbWZVdfU1PS54DVrYMoU2H9/WLy4zzcnIhJb3Qpwd0+5+wxgKnA8cFiu1Tq57kPuXuXuVZWVHb5I1GMLF0JDA6RS8NRTfb45EZHY6tFeKO6+DfgdMAsYY2Yt3+ScCgzKoEZDQ+b87t2DcY8iIvmpO3uhVJrZmOh8OXA6sApYDPxFtNoc4JmBKjLbnj2Z801Ng3GPIiL5qTu/hTIJmG9mhYTA/3d3X2hmK4FHzew2YCnw8ADW2Sq7684OcxGRpOkywN19OTAzx/I1hPHwQZUd2gpwEUmy2H0TU0MoIiJB7AJcQygiIkHsAlwduIhIELsAVwcuIhLELsDVgYuIBLEOcHXgIpJksQvw7CEUdeAikmSxC3B14CIiQewCXBsxRUSC2AW4NmKKiASxDnB14CKSZLELcG3EFBEJYhfg6sBFRILYBbg6cBGRIHYBrg5cRCSIdYCn0+HYmCIiSRS7AG9/HEwNo4hIUsUuwNsPm2gYRUSSKlYBnkqFYZNs6sBFJKliFeDth09AHbiIJFesAjxXWKsDF5GkilWAqwMXEcmIVYCrAxcRyYh9gKsDF5Gk6jLAzWyamS02s1Vm9o6ZXRMtn2dm68xsWTSdPdDFaghFRCSjqBvrNAM3uPubZjYSeMPMFkWX3ePudw5ceW1pCEVEJKPLAHf3DcCG6PwOM1sFTBnownJRBy4iktGjMXAzmw7MBF6LFn3bzJab2SNmNraT61xhZtVmVl1TU9OnYtWBi4hkdDvAzWwE8ARwrbtvBx4EDgJmEDr0u3Jdz90fcvcqd6+qrKzsU7HaiCkiktGtADezYkJ4/8rdnwRw943unnL3NPAvwPEDV2aQawhFHbiIJFV39kIx4GFglbvfnbV8UtZqFwAr+r+8ttSBi4hkdGcvlBOBy4G3zWxZtOwm4FIzmwE4sBb45oBUmEVj4CIiGd3ZC+W/Actx0Qv9X87eaS8UEZEMfRNTRCSmYhXg2ogpIpIRqwBXBy4ikhH7AFcHLiJJFasA10ZMEZGMWAW4OnARkYxYBbg6cBGRjFgFuDpwEZGM2Ae4OnARSapYBbiGUEREMmIV4BpCERHJiFWAqwMXEcmIVYCrAxcRyYh9gKsDF5GkilWA68esREQyYhXg6sBFRDJiH+DqwEUkqWIV4NoLRUQkI1YBriEUEZGMWAW4NmKKiGTEKsDVgYuIZMQ+wNWBi0hSxSrAtRFTRCQjVgGuDlxEJKPLADezaWa22MxWmdk7ZnZNtHycmS0ys9XR6diBLlYduIhIRnc68GbgBnc/DJgFXGVmhwPfB15294OBl6P5AZNKQTrdcbk6cBFJqi4D3N03uPub0fkdwCpgCnAeMD9abT5w/kAVCZ132urARSSpejQGbmbTgZnAa8BEd98AIeSBCZ1c5wozqzaz6pqaml4Xmj18Mnx45rwCXESSqtsBbmYjgCeAa919e3ev5+4PuXuVu1dVVlb2pkagbVCXl2fOp9NheEVEJGm6FeBmVkwI71+5+5PR4o1mNim6fBKwaWBKDLIDvLQUSkoy8xoHF5Ek6s5eKAY8DKxy97uzLnoWmBOdnwM80//lZWQPoZSWQnFxZl4BLiJJ1J0O/ETgcuBUM1sWTWcDdwCzzWw1MDuaHzDZHXhJSdsOXOPgIpJERV2t4O7/DVgnF5/Wv+V0LrsDbx/g6sBFJIli803M9mPg2UMo6sBFJIliGeDqwEVEYhTge9uIqQ5cRJIoNgGujZgiIm3FJsDbb8TUboQiknSxCfC9fZFHHbiIJFEsA1wduIhIjAK8/UZMdeAiknSxCXDtRigi0lZsA1y7EYpI0sUmwDWEIiLSVmwCXBsxRUTaik2AqwMXEWkrNgGuDlxEpK3YBrg6cBFJutgE+N6GUNSBi0gSxSbAtRuhiEhbsQlwHZFHRKSt2AS4jsgjItJWLANcGzFFRGIU4Hs7Io+GUEQkiWIT4OrARUTaik2A64g8IiJtxSbAdUQeEZG2ugxwM3vEzDaZ2YqsZfPMbJ2ZLYumswe2TP0euIhIe93pwH8BnJlj+T3uPiOaXujfsjra20ZMdeAikkRdBri7vwJsHYRa9koduIhIW30ZA/+2mS2PhljGdraSmV1hZtVmVl1TU9PrO9NX6UVE2uptgD8IHATMADYAd3W2ors/5O5V7l5VWVnZy7vT74GLiLTXqwB3943unnL3NPAvwPH9W1ZH+j1wEZG2ehXgZjYpa/YCYEVn6/YXdeAiIm0VdbWCmS0ATgEqzOxTYC5wipnNABxYC3xzAGsEtBFTRKS9LgPc3S/NsfjhAailU6lUmADMoLBQGzFFRGLxTcz238I0UwcuIhK7AG8JbnXgIpJ0sQjw9hswQR24iEgsAlwduIhIR7ENcO1GKCJJF4sAzzWEoi/yiEjSxSLA1YGLiHS0TwS4OnARSaJYBHiuIZTCwsyyVArS6cGtSURkqMUiwHN14Poyj4gkXSwCPFcHDtqVUESSLRYBnqsDb39eHbiIJE2sA1wduIgkWSwCvLMhFO1KKCJJFosA1xCKiEhHsQhwbcQUEekoFgGuDlxEpKNYB7g6cBFJslgEeHc2YqoDF5GkiUWAqwMXEekoFgGuDlxEpKNYBLg6cBGRjmId4Poij4gkWSwCXEMoIiIddRngZvaImW0ysxVZy8aZ2SIzWx2djh3IIjWEIiLSUXc68F8AZ7Zb9n3gZXc/GHg5mh8w+iKPiEhHXQa4u78CbG23+DxgfnR+PnB+P9fVhr5KLyLSUW/HwCe6+waA6HRC/5XUkTpwEZGOBnwjppldYWbVZlZdU1PTq9tQBy4i0lFvA3yjmU0CiE43dbaiuz/k7lXuXlVZWdmrO9NuhCIiHfU2wJ8F5kTn5wDP9E85uWkIRUSko+7sRrgAeBU4xMw+NbOvA3cAs81sNTA7mh8wGkIREemoqKsV3P3STi46rZ9r6ZQ6cBGRjmL9TUx14CKSZLEIcHXgIiIdxTrA1YGLSJLFIsD1Y1YiIh3FIsC1H7iISEexC3BtxBQRCfI+wNNpaG4O582gsDBzmYZQRCTJ8j7A2w+fmGXm1YGLSJLlfYB3tgET1IGLSLLlfYB3tgET1IGLSLLFKsDVgYuIZOR9gGcPoagDFxHJyPsA39sQijpwEUmyvA/w7m7EVAcuIkmT9wGujZgiIrnFOsA1hCIiSZb3Ab63IRR14CKSZHkf4OrARURyy/sAVwcuIpJb3ge4OnARkdz2mQBXBy4iSdPlUemH2sUXw5e+FIZSitpVm/3TsqlU+OnZgrz/SBIR6R95H+BFRTBqVO7LzEIX3tJ9NzV1HCcXEdlXxb5f1YZMEUmq2Ae4NmSKSFL1aQjFzNYCO4AU0OzuVf1RVE+oAxeRpOqPMfAvuPvmfridXlEHLiJJFfshFHXgIpJUfQ1wB14yszfM7IpcK5jZFWZWbWbVNTU1fby7jtSBi0hS9TXAT3T3Y4CzgKvM7OT2K7j7Q+5e5e5VlZWVfby7jvRlHhFJqj4FuLuvj043AU8Bx/dHUT2RPYSiDlxEkqTXAW5mw81sZMt54AxgRX8V1l3qwEUkqfqyF8pE4Ckza7mdf3P3X/dLVT2gjZgiklS9DnB3XwMc3Y+19Io2YopIUmk3QhGRmIp9gKsDF5Gk2qcCXB24iCRJ7ANcuxGKSFLFPsA77cDnzRvsUkREBlXsA7zNRszaXZmZW29tu6ICXUT2MbEP8JLC5tbzTdd/F6ZOhbPOCgtefBG2bQvn2we65Nb+gy5rPpWChptvY/t22LIFNm6Ebd+/o+3QVfb19aEpMrDcfdCmY4891vvV3Ll+Dfc4uIP73VzrrTPZ04QJ4fTjj9tcN5ctW9xffTWc9kRzs/v8+e5HHeU+qrTBZ850v/hi97/7O/f/+MvHfPPmtnXnjXa1rGc/f+JXDX7D17b4SUfV+iGs8kkVu314eXPOp7ZlKi52HzvW/The869+1f3uu91f5Iv+6qvub73lvnq1+/rrf+o7d7qn07nve1Dne3rdvurP2vp6X4NpoJ/HhACqPUemxjvA3f07ZyxrDZHTT0t7w9ur3R9/PCyYOjV32lx+efTQg3Ta/b+++rBfdpl7aWm4qKjI/Zxz3H/5S/cdN97udXUhhP74R/dFi9yrq93XrnXfvt39scfcDz2083ADdzP3qir3G290X8DF/uyz4Xb+8Af3t771M1+71n3rVvemJveGm3/oW7a4f/KJ+3vvhftdu9Z93Tr3mpqwTn/YsMH9Wc71W/+2zs8/YrV/pnT9Xh9Df02FBSkfO3KPT2eNH3PoTp/9uTq/+IwtfjX3+j3Xf+zP3Pm+v73gbd/KGG96fan78uXuK1eGKy9d6v766+GFAPclS9zffDOsA+6bN2eeIAifrDt2hOXg3tgYXvCs17913b3N9/XDoyf319daBvKx9PVx9/fz2J+1DvZ8D3QW4BYuGxxVVVVeXV3dr7f5m3Pv58znr26dP/10ePppGD7CwJ3mnY289vAKaq+dR3NBKak0NFHMRiay7pBTWTfqcJZu/QyrPizZy73knzFjoKICKho/YdJx05g0iTAteYbxXzuPceNg3DgY/fDdFH/veoqLw/aClSvh+efh+eedpUutR/dppCllN8U0UUwThaTYTSm7GE5qAI+PXU49o9jOeLYwhXWtU5oCNlPBFsazhfE0U4ThFJCmoABGpusYwzbGUstYahnDttZpNHWMHO4ML21mRFkz5es/gKNn4CNG4iNHkf71b0hd9tc0DxtFc/lICu+7izHzrmNU6W4KCoDvfQ9+8pNwZO2CArjhBvj5z2H48DCdcw5ceSW89Rbpt9/BdtRhpaVh3cJC2LkTpk2D8vIwvfUWzJ4drvv00/C1r4Ut9CUlcP/9cMstYb2ysnBf//iPmSfo298Ol9fWwtatsGABXH45DBsWrnPvveHyljfBTTfBAw+E2yorg698BZ57LhwRvKwMTj4Zli/P3N+0abB+fbivyZPhnXdIb6mlbt1Oai+9kqb7HqSpsIzmwhK48krG3v/3jG3cwMhd/4PdOi/cd2lpmL773fC8pVJhuuUWuPPOzGP91rfgvvtg9+6wV8Itt8APfhDWTafhRz+C228PdZWWhsc+f37m+hddFB5LQUF4DV58MVzPHc49FxYtCtcrKYFZs8IfRElJWDZtWhgXLCgIr2tFRXidiovD0dULC8PttP5BWN/me8DM3vAcRzyLfYD78Z/jlte/zD9wc+uyE0+EOz77CE+M/hoLFoTXpCem8TGfsH+PaxlFHTdwF3OYz6dM5X0+y7scyiuczBKOJ01hj29zsJXRwHEnlXHCCcYJJ8CfnXc4oz9dyahRIVsKCg0aGsIg+ObNMGMG/txC9tTuonbjHlZ+5xFWXDSPFevH8uGrm9g1chK7dqSoZxg7GcEORtLAsKF+mL1mpBnF9tYPgZbTchooJBU+PEhTy1jWM5n1TGYjEzGcYdQzjHrKacCx6GMwTGkKcIw0BRhOKbspo5FyGiijMWvNJkrYQzkNbW5rK+NaP8gaKaOQVOtUwh7KaGy9zRL2UERz65SmgCaKW5cArY/FcPZQQiNlNFJGPcNaPzS7+tAuoomR7KCYptb7KmEPw6hnOLsYRj2l7KaEPa2PrZwGRrCT4exiBDsZRn3r81BOA0U0tz7HBaSzHmWYWp6f7KmU3a2NR8vjKiSF4W2e9xSF7KGEJopbT5uznqkUhXhJGV5cghcVY3W1FE6spLDIKCqCoj99QNEhf0ZRoVNc5NjyZdisWVhJEVZSgv3nS/yv1NuhAejp+26fDPDmZhg5Ehobuf3mem65vbzXNzWcnfwf/o1v8s8cy5us4QAe5RIWcCkrOJJh7GICm6ikhuHsopZxbCmawOb0eEak67iCh7iBuxh3yRfh0Ufhd78LAVdTA1deybYHF/DbtQey+N1JbHzmVerH70/Dll3UM4wdjGQ7o6hjNDsYSTFNrX/sZTTiGM0U0UQxuyllG2Pwftj+XEQTn+M1jv3mccycVcrMmXDYjBJKPGt/zL50GZ1c1tQE27fDtoqD2LrkQ7ZsCZ8H6y/7Dmu+9VPWrIEPP4SaD+vYYaN727SI5J16yimnEebO7dFG/s4CfOD+7x0M770HjY0wfTo331bO8Aq47rqOq02aBDNnhv+CWqaKf3+AKbdfxeTJYceVz82exMjlf4SP9oPzzuPAx+7gpokTuWlCEc2HF1F0yV+GYM4W7QDjQOtgxCGHhNPPfz6z3pVXMuZbl3AhcCGA7Q+b2wVbOg3bt+Pr38cOPwwuuACeeirnw05RQC1j2cJ4NjGBDUxiA5Nau72tjGudtjOqTac3mjpmFy3mnIvKmX3P2YyefBL8U1Ytc29qe2dz5/Z+vv1lkeJiGD8exrOGg47LuuCyO+HBn2Y9L2NINzv19SHwN02ZwbqFy1i3LvxHX3TrLYz/2W1UVITbKz7tJPz3/4V7+I97+2nns+3nT1NbG0YX6n54H7WXX8O2bbDtuVfYdczJ7NwJu3ZB/bqt2LhxmEWjIps3UjR5Yuv7pWnNx9SN2p/t23M+pEQaNQrGbl9L6Wentz5P6eVvs23qkWzdCvX1Q11h/rGGBijrv9uLd4C/+WY4PeYYAK69Nvybf/XVYRjrwgvDUOBpp4XhqzYOq4E2WbUTjjwyTAB/9VetlxSRCuOKCxaEBWbhr/7dd2HFCmzOnDBel71TerauQq/lNkePxkaPDvNPPtn2spY2NJWisKiIivdfpWLrVg6ZNSusW18fhjb+5qvw4INQGI3lfeMb8IUvwOLFmdtrBh4DDp3bsZa97EbY4/n2l/Xiw6CgAEaMCNPkuecz45zsFYrgyuz1T4PsY0LNnQF/nTVfUAstJc37LczLWnne/e1qf7DtvH0G6pzmZqirg7qKA9n2xhrq6sKeqrsvvITULx8lnQ4fHiO/ehFTXn2CyZNhv/2g4La/p+E7PwgfFvVQcP+9FH/nWkpKQvAV/PhHFNx8Y3ipb/8H9vztTTQ0hP6k8e6f0fQ3/5empvA2233SaTQ89zINDeEl9zlzGPfsfMaPD9s9hv/z3aSuub51mLnp3gdo/PpVNDaG9Ztnn0nTs7+muTl8e7nw4osoevqJ1qFevngG6RdfIp0OfUXpl2ZT9sqiMGR+/JGMX/c248eHYWPsAHgvuxk5Cj4J83v2wI5bfkzzDd+juTn8w9w4/RAalr5HfX34E9p9xrk0PbmQpqZQS8Nl32DnPf/Kjh1h+LnhJ/fT8I2rW5+L1DurSH/2sNbaCl54jsLzvkRhYXi7Nz/+FHvOvoA9e2DPf/6ePbM+z+7d0ZD6ytX4QQe3DqenP/6Egv2ntQ55F360mpLDD6akJBr2fv2PFJ98AsXFIT8KX3oBO/vs1g95X7iQ1JnnkkqFx5Za/HuaT/h862PxZcvwo2ZkNt+vWEFBwRH0q1xbNgdq6ve9UK67Ljw3P/xhm8U7doSdDXqkJ1uvu9rK31N9ub+e7k3R11qTahD3OOhzLX29/kDuftnfz2PC90KJd4Cfckp4CM8/37+325XB3s92IHfrEpG811mAx3cjpjuMHRv+n12/Pgx0S9fmzdM3JEViprONmPH9Kv1HH4Xw3m8/hXdPKLxF9hnxDfCWDZgzZw5tHSIiQyS+Ab50aThVgItIQsU/wKNdCEVEkia+Aa4hFBFJuHgG+IYN4QdORo+GAw4Y6mpERIZEPAM8e/zbevaLeiIi+4o+BbiZnWlm75nZB2b2/f4qKqfs3d80fCIi0vsAN7NC4AHgLOBw4FIzO7y/Cmtj69ZwSLSamjAtWRKWawOmiCRYX37M6njgA3dfA2BmjwLnASv7o7A2TjopnE6Y0Ha5OnARSbC+DKFMAT7Jmv80WtaGmV1hZtVmVl1TU9Oze5g3L4xxr+zkM+GII8Ll+nahiCRQXwI819bDDj+s4u4PuXuVu1dVVlb27B7mzcscSjHcWO55BbiIJFBfAvxTYFrW/FRgfd/KERGR7upLgL8OHGxmB5hZCXAJ8Gz/lJVDdw6KICKSIH36OVkzOxu4FygEHnH32/e2/kAc1FhEZF83IMfEdPcXgBf6chsiItI78fwmpoiIKMBFROJKAS4iElMKcBGRmBrUgxqbWQ3wp15evQLY3I/l9CfV1juqrXdUW+/EubbPuHuHb0IOaoD3hZlV59qNJh+ott5Rbb2j2npnX6xNQygiIjGlABcRiak4BfhDQ13AXqi23lFtvaPaemefqy02Y+AiItJWnDpwERHJogAXEYmpWAT4oB48uetaHjGzTWa2ImvZODNbZGaro9OxQ1TbNDNbbGarzOwdM7smX+ozszIzW2Jmb0W13RotP8DMXotqeyz6aeJBZ2aFZrbUzBbmU11RLWvN7G0zW2Zm1dGyIX9NozrGmNnjZvZu9L7783yozcwOiZ6vlmm7mV2bD7VF9V0X/R2sMLMF0d9Hj99zeR/gg3rw5O75BXBmu2XfB15294OBl6P5odAM3ODuhwGzgKui5yof6tsNnOruRwMzgDPNbBbwY+CeqLZa4OtDUBvANcCqrPl8qavFF9x9Rta+wvnwmgLcB/za3Q8FjiY8h0Nem7u/Fz1fM4BjgXrgqXyozcymAFcDVe5+BOHnuC+hN+85d8/rCfhz4DdZ8zcCNw5xTdOBFVnz7wGTovOTgPeG+nmLankGmJ1v9QHDgDeBzxG+fVaU67UexHqmEv6YTwUWEg4XOOR1ZdW3Fqhot2zIX1NgFPAR0c4Q+VRbu3rOAP6QL7WROZ7wOMJPei8Evtib91zed+B08+DJQ2yiu28AiE4nDHE9mNl0YCbwGnlSXzRMsQzYBCwCPgS2uXtztMpQvbb3At8F0tH8+Dypq4UDL5nZG2Z2RbQsH17TA4Ea4OfR8NO/mtnwPKkt2yXAguj8kNfm7uuAO4GPgQ1AHfAGvXjPxSHAu3XwZMkwsxHAE8C17r59qOtp4e4pD//STgWOBw7Ltdpg1mRm5wKb3P2N7MU5Vh3K99yJ7n4MYRjxKjM7eQhryVYEHAM86O4zgV0M3VBOTtE48peB/xjqWlpE4+7nAQcAk4HhhNe2vS7fc3EI8DgcPHmjmU0CiE43DVUhZlZMCO9fufuT+VYfgLtvA35HGKcfY2YtR4Yaitf2RODLZrYWeJQwjHJvHtTVyt3XR6ebCOO4x5Mfr+mnwKfu/lo0/zgh0POhthZnAW+6+8ZoPh9qOx34yN1r3L0JeBI4gV685+IQ4IN78OTeeRaYE52fQxh7HnRmZsDDwCp3vzvroiGvz8wqzWxMdL6c8CZeBSwG/mKoanP3G919qrtPJ7y3fuvuXxnqulqY2XAzG9lynjCeu4I8eE3d/X+AT8zskGjRacDKfKgty6Vkhk8gP2r7GJhlZsOiv9mW563n77mh3LjQg0H/s4H3CWOmNw9xLQsI41ZNhA7k64Qx05eB1dHpuCGq7X8T/u1aDiyLprPzoT7gKGBpVNsK4AfR8gOBJcAHhH9zS4fwtT0FWJhPdUV1vBVN77S8//PhNY3qmAFUR6/r08DYPKptGLAFGJ21LF9quxV4N/pb+H9AaW/ec/oqvYhITMVhCEVERHJQgIuIxJQCXEQkphTgIiIxpQAXEYkpBbiISEwpwEVEYur/A48ygoY7InvdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lin_reg=LinearRegression()\n",
    "plot_learning_curves(lin_reg,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at the learning curves of the 10 degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pipeline' from 'sklearn.pipeline' (/Users/shivambali/opt/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-c0190ac1fa48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m polynomial_regression=pipeline((\n\u001b[1;32m      3\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'poly_features'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPolynomialFeaturs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minclude_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'lin_reg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ))\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'pipeline' from 'sklearn.pipeline' (/Users/shivambali/opt/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import pipeline\n",
    "polynomial_regression=pipeline((\n",
    "('poly_features',PolynomialFeaturs(degree=10,include_bias=False)),\n",
    "('lin_reg',LinearRegression()),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bias/Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning the errors are categorized in 3 catogories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias=This part of genelrization error occurs due to the wrong assumption.A high bias is most likely to underfit the training data<br>\n",
    "Variance=This part of generization error is cause due to to the hogh sensitivity to small variation in the training det.a model with degree of freedom is likely to have high variance,And thus overfit the training data<br>\n",
    "Ireeducible error=this error is due to the high noise in data intself. the oly way to reduce this type of error is to clean us the dara."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we reduce the overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to reduce the overfitting is to regularize the model. The feawer the freedom of deegre harder will be to overfit for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Linear regression is typically achieved by constraining the weights of the model. It can be done by Ridge regression,Lasso Regression and elastic nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RIdge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression is regularized version of Linear Regression. It is added in the cost function during the training. and models performance should be evaluated using the unregularized measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypermeter alpha controls how much regularizaton should be sine.If alpha=0 then rodge regression is just plain linear regression.And if the alpha is very large then the all weights end up very close to zero and the result is the g=flat line going throgh the data mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='ridge.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression uses l2 norm of the wieght vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression closed form solution is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='nridge.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regresson in scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.72951989]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg=Ridge(alpha=1,solver='cholesky')\n",
    "ridge_reg.fit(X,y)\n",
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.85711177])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sgd_reg = SGDRegressor(penalty=\"l2\")\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penalty l2 indiacates that you wan to use regularization term to the cost function equal to the half of the l2 morm of the weight vector. This is simply ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another regularized version of the Linear Regression.it adds a regularization term to the cost function,but it uses the l1 norm of the weight vector instead of the half the square of the l2 norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='lasso.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature of lasso regression that it eliminates the weight of the least important features so it automatically performs the feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.6308914])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg=Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X,y)\n",
    "lasso_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso using lasso regression. Penality =l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.85029944])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sgd_reg = SGDRegressor(penalty=\"l1\")\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic NET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic net is the middle ground between Ridge and lasso regression. It is the combination of the Lasso and ridge. WE can control the mix of the regularization.Whe l=0 then it is ridge and when r=1 is lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should we use between Ridge, Lasso and Elstic net?<br>\n",
    "It is good to use some regularization. Therefore ridge is good default. But if some features are reduntant then we should use Lasso since it computes feature elimination<br>\n",
    "But elastic is better then lasso since lasso behaves erratic when the several features are strongly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.64728048])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net=ElasticNet(alpha=0.1,l1_ratio=0.5)\n",
    "elastic_net.fit(X,y)\n",
    "elastic_net.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l1 ratio corresponds to the mox ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EARLY STOPPING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to regularize iterative learning algorithms such as GD is to stop the training as soon as validation error is at the minimum.This is called early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_iter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-8a791e8d0db7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m sgd_reg = SGDRegressor(n_iter=1, warm_start=True, penalty=None,\n\u001b[0;32m----> 3\u001b[0;31m learning_rate=\"constant\", eta0=0.0005)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mminimum_val_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_iter'"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.base import clone\n",
    "sgd_reg = SGDRegressor(n_iter=1, warm_start=True, penalty=None,\n",
    "learning_rate=\"constant\", eta0=0.0005)\n",
    "minimum_val_error = float(\"inf\")\n",
    "best_epoch = None\n",
    "best_model=None\n",
    "for epoch in range(1000):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train) # continues where it left off y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    val_error = mean_squared_error(y_val_predict, y_val)\n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error \n",
    "        best_epoch = epoch\n",
    "        best_model = clone(sgd_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the warm_start=True,it continues trainng where it left instead of restarting from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logsitic regression is a common model used to evaluate the probabilites that an instace belongs to a certain class. If the probability intsance is greater than 50% then the model is classified as a positive class. for less then 50% it belongs to the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESTIMATING PROBABILIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also works like the simple regression model. it gives the weighted sum of the input feature but instead of directly outputting the weighted value the sigmoid function in logistic regression outputs in between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='sigmo.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='logit.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How logistic regression model is trained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the training is to sset the theta so that the model estimates the highest probabilities for positive instance and negative instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='cost.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-log(t) approaches very large when t approaches zero,so cost will be very large if the model estimates close to 0 for a positive class.And it will be very large if the model predicts 1 for the negative instance.Similarly -log(t) will be closer ot zero if it predicts 0 for the negative class, also be 0 if it predicts 1 for the positve class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function over the whole training set is just the average cost of all instances. it can be written as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='costall.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad news is that there is no normal equation or closed form eq to find the value of theta that minimizes the cost function.But the logit regression is convex so ot os guatanteed that the GD will fond the optimal theta that minimizes the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECISION BOUNDARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())\n",
    "['data', 'target_names', 'feature_names', 'target', 'DESCR']\n",
    "X = iris[\"data\"][:, 3:] # petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.int) # 1 if Iris-Virginica, else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a1a826910>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris-Virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris-Virginica\") # + more Matplotlib code to make the image look pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary is at 1.6. So anything above 1.6 belongs to green class and below 1.6 belongs to blue dotted class.Accordong to model below 1cm it is confident in blue class and above every 2 it is confident in green class. and between 1 and 2 it seems confused.However if the ask the model to predict class usinf predict() rather then asking predict_proba(),It will return whichever class is most likely.Therefore the desicios boundary is at 1.6 where both probabilities is at 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just like other linear models,logistic regression models can be regularized using both l1 and l2 penalties.Scikit learn adds l2 norm by default<br>\n",
    "the hyperparemet controlling the regularization strength in logistic regression model is not alpha but the inverse of the C. the higher the C less the model is regualrized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logostic Regression model can be generized to support multiple class directly wothout having to train multiple binary classifier.This is called softmax regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='softmax.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='softmax1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='123.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like Logostic regression classifer,the softmax regression classifier predict the highest probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='srcp.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argmax function returns the value of the variable that maximizes a function. It returns the value of K that maximizes the estimated probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax regression classifier predict only one class at a time. There it can be used to classiy the type of plant but not to recognize the multiple people in one picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model estimates for soft max?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizing the cost function Called the cross entroopy.It oenalizes the model when it estimates a low probability for a target class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='ce.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src='ce1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lOGISITIC  regression usses onevsall by default when you train on more than two classes.But we can use multi_class hyperparameter to 'multinomial' to switch it to softmax regression instead.We need to specify solver that supports softmax such as 'lbfs' slover.It also applies l2 regularization by default which we can contol using hyperparameter c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, multi_class='multinomial')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width y = iris[\"target\"]\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10)\n",
    "softmax_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-6a7b863758f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msoftmax_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msoftmax_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'array' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "softiimax_reg.predict([[5, 2]])\n",
    "array([2])\n",
    "softmax_reg.predict_proba([[5, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
